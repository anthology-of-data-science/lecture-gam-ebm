---
title: Generalized Additive Models and Explainable Boosting Machines
subtitle: Your standard algorithm for tabular data?
format: clean-revealjs

html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Dr Daniel Kapitan
    orcid: 0000-0001-8979-9194
    email: daniel@kapitan.net
    affiliations: Eindhoven AI Systems Institute
date: last-modified

---

## Attribution & copyright notice

<br>

::: {style="font-size: 80%;"}

This lecture is based on the following open access materials:

- James et al., [Introduction to Statistical Learning, chapter 6](https://www.statlearning.com/)
- Rich Caruana, [InterpretML: Explainable Boosting Machines (EBMs)](https://people.orie.cornell.edu/mru8/orie4741/lectures/Tutorial4MadeleineUdellClass_2020Dec08_RichCaruana_IntelligibleMLInterpretML_EBMs_75mins.pdf)


Source code: [https://github.com/anthology-of-data-science/lecture-gam-ebm](https://github.com/anthology-of-data-science/lecture-gam-ebm)
<br>



```{=html}
<p xmlns:cc="http://creativecommons.org/ns#" >Daniel Kapitan, <em>Generalized Additive Models and Explainable Boosting Machines.</em><br>This work is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>
```
:::
## Learning objectives: moving beyond linearity

<br>

:::: {.columns style="font-size: 80%;"}
::: {.column width="45%"}
### Generalized Additive Models

- Know how to use additive models with a single feature
  - polynomial regression
  - regression splines
  - smoothing splines
- Know how to use generalized additive models with multiple features
  - for regression
  - for classification
  
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
### Explainable Boosting Machines

- Know how to use Explainable Boosting Machines by
  - training smoothed splines
  - correcting the learned splines
  - interpreting EBMs
- Reflect on usefulness of EBMs for high-risk applications
:::
::::

# Generalized Additive Models
## Moving beyond linearity

<br>

:::: {.columns style="font-size: 80%;"}
::: {.column width="40%"}

### From linear regression

$$
y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
$$

<br>

### From logistic regression

\begin{align}
p(x)& = \frac{e^{y_{i}}}{1 + e^{y_{i}}}\\
\log \left( {\frac{p(x)}{1 - p(x)}} \right) & = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
\end{align}
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
### ... to polynomial regression

$$
y_{i} = \beta_{0} + \beta_{1} x_{i} + \beta_{2} x_{i}^{2} + \beta_{3} x_{i}^{3} + \ldots +\epsilon_{i}
$$
<br>

### ... to logistic polynomial regression

\begin{align}
p(x)& = \frac{e^{y_{i}}}{1 + e^{y_{i}}}\\
\log\left({\frac{p(x)}{1 - p(x)}}\right)& = \beta_{0} + \beta_{1} x_{i} + \beta_{2} x_{i}^{2} + \beta_{3} x_{i}^{3} + \ldots +\epsilon_{i}
\end{align}
:::
::::


## Polynomial regression

<br>

- Generalization of linear regression, use ordinary least squares for estimating coefficients
- Unusual for d > 3 or 4, because curve becomes too flexible
- Classification usually uses logit or log-odds formula

<br>

- Polynomials are an example of using basis functions
  - Fourier basis is other commonly used basis function (sin, cos) for periodic functions
