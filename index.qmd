---
title: Generalized Additive Models and Explainable Boosting Machines
subtitle: Your standard algorithm for tabular data?
format: clean-revealjs

html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Dr Daniel Kapitan
    orcid: 0000-0001-8979-9194
    email: daniel@kapitan.net
    affiliations: Eindhoven AI Systems Institute
date: last-modified

---

## Attribution & copyright notice

<br>

::: {style="font-size: 80%;"}

This lecture is based on the following open access materials:

- James et al., [Introduction to Statistical Learning with Python (ISLP), chapter 7](https://www.statlearning.com/)
- Rich Caruana, [InterpretML: Explainable Boosting Machines (EBMs)](https://people.orie.cornell.edu/mru8/orie4741/lectures/Tutorial4MadeleineUdellClass_2020Dec08_RichCaruana_IntelligibleMLInterpretML_EBMs_75mins.pdf)


Source code: [https://github.com/anthology-of-data-science/lecture-gam-ebm](https://github.com/anthology-of-data-science/lecture-gam-ebm)
<br>



```{=html}
<p xmlns:cc="http://creativecommons.org/ns#" >Daniel Kapitan, <em>Generalized Additive Models and Explainable Boosting Machines.</em><br>This work is licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">CC BY-SA 4.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>
```
:::
## Why this lecture?

::: {style="font-size: 60%;"}
![Source: Adaptation of ISLP, figure 2.7](images/xkcd-interpretability-flexibility.png)
:::

## Learning objectives: moving beyond linearity

<br>

:::: {.columns style="font-size: 80%;"}
::: {.column width="45%"}
### Generalized Additive Models

- Know how to use additive models with a single feature
  - polynomial regression
  - regression splines
  - smoothing splines
- Know how to use generalized additive models with multiple features
  - for regression
  - for classification
  
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
### Explainable Boosting Machines

- Know how to use Explainable Boosting Machines by
  - training smoothed splines
  - correcting the learned splines
  - interpreting EBMs
- Reflect on usefulness of EBMs for high-risk applications
:::
::::

# Generalized Additive Models
## Moving beyond linearity

<br>

:::: {.columns style="font-size: 80%;"}
::: {.column width="40%"}

### From linear regression

$$
y_{i} = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
$$

<br>

### From logistic regression

\begin{align}
p(x)& = \frac{e^{y_{i}}}{1 + e^{y_{i}}}\\
\log \left( {\frac{p(x)}{1 - p(x)}} \right) & = \beta_{0} + \beta_{1} x_{i} + \epsilon_{i}
\end{align}
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
### ... to polynomial regression

$$
y_{i} = \beta_{0} + \beta_{1} x_{i} + \beta_{2} x_{i}^{2} + \beta_{3} x_{i}^{3} + \ldots +\epsilon_{i}
$$
<br>

### ... to logistic polynomial regression

\begin{align}
p(x)& = \frac{e^{y_{i}}}{1 + e^{y_{i}}}\\
\log\left({\frac{p(x)}{1 - p(x)}}\right)& = \beta_{0} + \beta_{1} x_{i} + \beta_{2} x_{i}^{2} + \beta_{3} x_{i}^{3} + \ldots +\epsilon_{i}
\end{align}
:::
::::


## Polynomial regression
<br>

### Generalization of linear regression
- Use ordinary least squares for estimating coefficients
- Unusual for d > 3 or 4, because curve becomes too flexible
- Classification usually uses logit or log-odds formula

<br>

### Concept of basis functions
- Polynomials are an example of basis functions
- Fourier basis is other commonly used basis function (sin, cos) for periodic functions


## Example: `Wage` data

::: {style="font-size: 60%;"}
![Source: ISLP, figure 7.1](images/islp-7_1.png)
:::

## Regression splines
### Not as many extra degrees of freedom as you may think

:::: {.columns}
::: {.column width="60%" style="font-size: 60%;"}
![Source: ISLP, figure 7.3](images/islp-7_3.png)
:::

::: {.column width="40%" style="font-size: 80%;"}

__Piecewise cubic__: 2 x 4 coefficients <br>
--> 8 degrees of freedom (DoF) 

<br>

__Continuous cubic (no gaps)__: one extra constraint --> 7 DoF

<br>

__Cubic spline__: require 1st and 2nd derivative to be continuous --> two extra constraints --> 5 DoF
:::
::::

## Smoothing splines
### Determine set of knots with regularization

<br>

$$
{\color{green}\sum^{n}_{i=1} {\left(y_{i} - g(x_{i}) \right)}^{2}} + {\color{#ff4f5e} \lambda \int g''(t)^{2}dt}
$$

<br>

Same principle as Lasso and Ridge regression: ${\color{green} loss} + {\color{#ff4f5e} penalty}$ 

- Low $\color{#ff4f5e}\lambda$:  low penalty for 'wildly oscillating' function $g(x)$
- High $\color{#ff4f5e}\lambda$:  high penalty forces $g(x)$ to become smoother (hence the name)
- Selection of $\color{#ff4f5e}\lambda$ done with cross validation (usually LOOCV)

## Smooting splines
### Example: tricep skinfold thickness as a function of age

![Source: [Biostatistics Collaboration of Australia](https://bookdown.org/tpinto_home/Beyond-Linearity/smoothing-splines.html).](./images/ssplines.gif)

## Generalized additive models with multiple features
<br>

### From multiple linear regression:

$$
y_{i} = \beta_{0} + \beta_{1} x_{i1} + \beta_{2} x_{i2} + \ldots + \beta_{p} x_{ip} ++ \epsilon_{i}
$$

### ... to GAMs

\begin{align}
y_{i} &= \beta_{0} + f_{1}(x_{i1}) + f_{2}(x_{i2}) + \ldots + f_{p}(x_{ip}) + \epsilon_{i} \\
y_{i} &= \beta_{0} + \sum^{p}_{j=1} f(x_{ij}) + \epsilon_{i}
\end{align}

- __Generalized__: for each function $f_j$ you can choose which (non-)linear basis function you want to use
- __Additive__: we assume we can add contributions of each separate $f_j$

## GAM for `Wage` data
### Regression using natural splines
$wage = \beta_0 + f_1(year) + f_2(age) + f_3(education)$

:::{style="font-size: 80%;"}
$f_1$: four degrees of freedom, $f_2$: five degrees of freedom

![Source: ISLP, figure 7.11.](./images/islp-7_11.png)
:::

## GAM for `Wage` data
### Regression using smoothing splines
$wage = \beta_0 + f_1(year) + f_2(age) + f_3(education)$

:::{style="font-size: 80%;"}
$f_1$: four degrees of freedom, $f_2$: five degrees of freedom

![Source: ISLP, figure 7.12.](./images/islp-7_12.png)
:::


## GAM for `Wage` data
### Probability of earning more than 250 thousand dollars per year

$\log\left({{p(x)}/{1 - p(x)}}\right) = \beta_0 + beta_1 \times year + f_2(age) + f_3(education)$

:::{style="font-size: 80%;"}
$f_2$: five degrees of freedom

![Source: ISLP, figure 7.12.](./images/islp-7_13.png)
:::

## Pros and Cons of GAMS

:::: {.columns style="font-size: 80%;"}
::: {.column width="45%"}
$\color{green}\bigtriangleup$ You can fit a non-linear $f_j$ to each $X_j$, so we can automatically model such relationships (no need for manual transformation)

$\color{green}\bigtriangleup$ Using non-linear functions potentially results in more accurate predictions

$\color{green}\bigtriangleup$ Because model is additive, you can examine effect of each feature $X_j$ on response $Y$ individually

$\color{green}\bigtriangleup$ Smoothness of functions can be summarized via degrees of freedom
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}


$\color{orange}\bigtriangledown$ Additive model may be too restrictive, doesn't include interactions

$\color{orange}\bigtriangledown$ Can be computationally expensive for many features
:::
::::

# Explainable Boosting Machines
## Remember how gradient boosting works?

![Source: [Python Geeks](https://pythongeeks.org/gradient-boosting-algorithm-in-machine-learning/)](./images/working-of-gradient-boosting-algorithm.png)

## Generalized additive models with pairwise interactions ($GA^{2}M$)
### Microsoft Research implemented it and branded it as EBM

$$
g(E[y]) = \beta_0 + {\color{#00458b} \sum f_i(x_{i})} + {\color{#6e008b} \sum f_{ij}(x_{ij})}
$$

:::: {.columns style="font-size: 80%;"}
::: {.column width="30%"}
$g(E[y]):$<br><br>link function, identity for regression, logit for logistic regression
:::
::: {.column width="5%"}
:::
::: {.column width="30%"}
${\color{#00458b} \sum f_i(x_{i})}:$<br><br>
GAM, but now using shallow trees as basis function
:::
::: {.column width="5%"}
:::
::: {.column width="30%"}
${\color{#6e008b} \sum f_{ij}(x_{i})}:$<br><br>pairwise interactions
:::
::::

:::: {style="font-size: 80%;"}
- Combines different ideas into single model ([Lou et al. (2019)](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/kdd13.pdf))
  - Fast detection of pairwise interactions
  - Uses gradient boosted trees, training one feature at a time
  - Cycles features with each iteration to mitigate effect of co-linearity
- Implemented in [InterpretML](https://github.com/interpretml/interpret/)
::::


## Intuition how EBM algorithm works

{{< video ./images/ebm.m4v >}}

## Intuition how EBM algorithm works (intermediate result)

![](./images/ebm-splines.jpg )

## Fast detection of pairwise interactions
### [Lou et al. (2013)](./resources/lou2013accurate.pdf)
![Searching cuts on input space of $x_i$ and $x_j$. On the left we show a heat map on the target for different values of $x_i$ and $x_j$. $c_i$ and $c_j$ are cuts for $x_i$ and $x_j$, respectively. On the right we show an extremely simple predictor of modeling pairwise interaction.](./images/fast-interaction-detection.png)

## Intuition how EBM algorithm works (final result)

![](./images/ebm-with-pairs.png)

## Performance of EBM on some datasets
### [Chang et al. (2020)](./resources/chang2020how.pdf)

![Test set AUCs (%) across ten datasets average over five runs. Best number in each row is in bold.](./images/ebm-performance.png)


# Example: predicting pneumonia mortality risk
## The Pneumonia Data with 46 features
### [Cooper et al. (1997)](./resources/cooper1997evaluation.pdf)

![The dataset contains 14,199 cases of pneumonia collected from 78 hospitals between July 1987 and December 1988.](./images/pneumonia-dataset.png)

## Using EBMs to detect common flaws in data
### [Chen et al. (2021)](./resources/chang2020how.pdf)
<br>

1. EBM shape function graphs can be helpful in identifying various types of dataset flaws.
2. In many cases, users with domain expertise are needed to examine what the model has learned.
3. In some cases, EBMs provide simple tools for correcting problems in the models, when correcting the data is not feasible or too difficult.

## Missing values assumed normal
### [Chen et al. (2021)](./resources/chang2020how.pdf)
<br>

![EBM shape function of “heart rate” for predicting pneumonia mortality risk. Left: missing values result in unrealistic high risk score. Right: corrected risk score.](./images/pneumonia-heart-rate.png)

## Correction for confounders and treatment effects
### [Chen et al. (2021)](./resources/chang2020how.pdf)
<br>

![Left: confounder of retirement at age 67, resulting in sharp increase of risk. Social effect of doctors trying harder to cure centenarians results in lower risk. Right: patients who have a history of asthma have lower pneumonia mortality risk than general population, since they admitted directly into ICU and get more aggressive care, thereby lowering their risk of death.](./images/pneumonia-age-asthma.png)

## Discovering new protocols?
### [Chen et al. (2021)](./resources/chang2020how.pdf)
<br>

![Left: patients get treated when blood urea nitrogent reaches ~50. When BUN goes over 100, dialysis is given. Right: patients in ICU get treated at systolic blood pressures (SBP) of 175, 200 and 255.](./images/pneumonia-treatment-effects.png)

## Discovering new protocols?
### [Chen et al. (2021)](./resources/chang2020how.pdf)
<br>

![Left: possible improvement by moving dialysis treatment to 80. Rightpatients get treated when blood urea nitrogent reaches ~50. When BUN goes over 100, dialysis is given. Right: adjust "inappropriate" treatment thresholds with flattend red lines.](./images/pneumonia-upper-bounds.png)

## Where to go from here?
### Try it yourself
<br>

- Work through one of the examples on [InterpreML](https://interpret.ml/docs/python/examples/interpretable-classification.html)
- Reproduce the results from [Chang et al. (2020)](./resources/chang2020how.pdf) with the
  - [Adult census income dataset](https://archive.ics.uci.edu/dataset/2/adult)
  - [COMPAS Recividism dataset](https://www.kaggle.com/datasets/danofer/compass)
- Read the paper by [Nori et al. (2021)](./resources/nori21accuracy.pdf) how EBMs can be combined with differential privacy (DP) to achieve state-of-the-art accuracy whilst preserving privacy
  

## Thanks for your attention. {background-image="images/speakerscorner.jpg" background-size="contain" background-repeat="no-repeat"}